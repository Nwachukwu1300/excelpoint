# AI Chatbot Response Caching System - Product Requirements Document

## Project Overview
Implement a lightweight caching system specifically for AI chatbot responses to reduce OpenAI API costs and improve response times for repeated questions within the same subject context.

## Problem Statement
Currently, every user question triggers a full RAG pipeline execution, including:
- Vector search through content chunks
- OpenAI API call for response generation
- Full processing overhead

This results in:
- High OpenAI API costs for repeated questions
- Slower response times for common queries
- Unnecessary computational overhead

## Solution
Implement a database-based caching layer that stores AI responses with intelligent cache keys and configurable expiration times.

## Core Requirements

### 1. Cache Schema Design
- **Cache Model**: Django model for storing cached responses
- **Cache Key Components**: user_id + subject_id + normalized_question
- **Storage Fields**: response_data, created_at, expires_at, hit_count
- **Indexing**: Optimize for fast lookups by cache key

### 2. Cache Integration
- **RAG Service Integration**: Modify RAG service to check cache before processing
- **Cache Hit Flow**: Return stored response immediately
- **Cache Miss Flow**: Process through RAG pipeline, store result, return response
- **Cache Write**: Store new responses with expiration time

### 3. Cache Management
- **Expiration**: Configurable TTL (24-72 hours default)
- **Cleanup**: Automatic removal of expired entries
- **Hit Tracking**: Monitor cache effectiveness
- **Size Management**: Prevent unlimited growth

### 4. Performance & Monitoring
- **Logging**: Track cache hits/misses for optimization
- **Metrics**: Response time improvements
- **Cost Tracking**: OpenAI API call reduction
- **Debugging**: Cache key generation and lookup debugging

## Technical Specifications

### Cache Key Generation
```
cache_key = f"{user_id}:{subject_id}:{normalized_question}"
normalized_question = question.lower().strip()
```

### Cache Entry Structure
```json
{
  "user_id": "user_identifier",
  "subject_id": "subject_identifier", 
  "question_hash": "md5_hash_of_normalized_question",
  "response_data": "full_rag_response_object",
  "created_at": "timestamp",
  "expires_at": "timestamp",
  "hit_count": "number_of_times_accessed"
}
```

### Integration Points
- **subjects/services/rag_service.py**: Main integration point
- **subjects/models.py**: Cache model definition
- **subjects/views.py**: API endpoint modifications
- **subjects/management/commands/**: Cache cleanup commands

### Configuration
- **CACHE_TTL_HOURS**: Default expiration time (24-72 hours)
- **CACHE_ENABLED**: Feature flag for enabling/disabling
- **CACHE_MAX_SIZE**: Maximum number of cached entries
- **CACHE_LOG_LEVEL**: Logging verbosity

## Success Metrics
- **Cost Reduction**: 60-80% reduction in OpenAI API calls for repeated questions
- **Performance**: 90%+ cache hit rate for common questions
- **Response Time**: <100ms for cached responses vs 2-5s for new queries
- **Storage Efficiency**: <1GB cache size for typical usage

## Implementation Phases

### Phase 1: Core Cache Infrastructure
1. Design and implement cache model
2. Create cache key generation utilities
3. Implement basic cache read/write operations
4. Add cache configuration settings

### Phase 2: RAG Service Integration
1. Modify RAG service to check cache before processing
2. Implement cache hit/miss logic
3. Add response storage after RAG processing
4. Integrate with existing chat API endpoints

### Phase 3: Cache Management & Monitoring
1. Implement cache expiration and cleanup
2. Add cache hit/miss logging
3. Create cache management commands
4. Add performance monitoring

### Phase 4: Testing & Optimization
1. Unit tests for cache operations
2. Integration tests for RAG service
3. Performance testing and optimization
4. Cache effectiveness analysis

## Non-Requirements
- Session data caching (handled by existing Django session system)
- User data caching (not in scope)
- Redis dependency (use database-based caching)
- Complex cache invalidation (simple TTL-based expiration)

## Dependencies
- Existing RAG service implementation
- Django ORM for cache storage
- Current chat API endpoints
- OpenAI API integration (existing)

## Risks & Mitigation
- **Cache Size Growth**: Implement size limits and cleanup
- **Stale Responses**: Use reasonable TTL and allow manual refresh
- **Performance Impact**: Monitor cache lookup performance
- **Data Consistency**: Ensure cache keys are deterministic

## Future Enhancements
- Cache warming for popular questions
- Intelligent cache invalidation based on content changes
- Cache analytics dashboard
- Multi-level caching (memory + database) 